{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.calibration import LinearSVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './data.tsv'\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_w2v_model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "King - Man + Woman = ('queen', 0.7118193507194519)\n",
      "Similarity between 'excellent' and 'outstanding': 0.5567486\n",
      "Fastest - Fast + Slow = ('slowest', 0.7025300860404968)\n"
     ]
    }
   ],
   "source": [
    "result = google_w2v_model.most_similar(positive=['king', 'woman'], negative=['man'], topn=1)\n",
    "print(\"King - Man + Woman =\", result[0])\n",
    "\n",
    "similarity = google_w2v_model.similarity('excellent', 'outstanding')\n",
    "print(\"Similarity between 'excellent' and 'outstanding':\", similarity)\n",
    "\n",
    "result = google_w2v_model.most_similar(positive=['fastest', 'slow'], negative=['fast'], topn=1)\n",
    "print(\"Fastest - Fast + Slow =\", result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jehil\\AppData\\Local\\Temp\\ipykernel_19908\\548431678.py:1: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  review_data = pd.read_table(DATA_PATH, on_bad_lines='skip', usecols=['star_rating', 'review_body'])\n"
     ]
    }
   ],
   "source": [
    "review_data = pd.read_table(DATA_PATH, on_bad_lines='skip', usecols=['star_rating', 'review_body'])\n",
    "review_data['star_rating'] = pd.to_numeric(review_data['star_rating'], errors='coerce')\n",
    "review_data.dropna(inplace=True)\n",
    "review_data['target'] = review_data['star_rating'].apply(lambda x: 1 if x >= 4 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_reviews = review_data[review_data['target'] == 1].sample(n=50000, random_state=RANDOM_SEED)\n",
    "negative_reviews = review_data[review_data['target'] == 0].sample(n=50000, random_state=RANDOM_SEED)\n",
    "final_data = pd.concat([positive_reviews, negative_reviews], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data['review_body'] = final_data['review_body'].str.lower()\n",
    "final_data['tokens'] = final_data['review_body'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 300\n",
    "window_size = 13\n",
    "min_word_count = 9\n",
    "\n",
    "amazon_w2v_model = Word2Vec(sentences=final_data['tokens'], vector_size=embedding_size, window=window_size, min_count=min_word_count)\n",
    "amazon_w2v_model.save('amazon_w2v_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "King - Man + Woman = ('author', 0.5631979703903198)\n",
      "Similarity between 'excellent' and 'outstanding': 0.87626386\n",
      "Fastest - Fast + Slow = ('warm-up', 0.4784909784793854)\n"
     ]
    }
   ],
   "source": [
    "result = amazon_w2v_model.wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=1)\n",
    "print(\"King - Man + Woman =\", result[0])\n",
    "\n",
    "similarity = amazon_w2v_model.wv.similarity('excellent', 'outstanding')\n",
    "print(\"Similarity between 'excellent' and 'outstanding':\", similarity)\n",
    "\n",
    "result = amazon_w2v_model.wv.most_similar(positive=['fastest', 'slow'], negative=['fast'], topn=1)\n",
    "print(\"Fastest - Fast + Slow =\", result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you conclude from comparing vectors generated by yourself and the pretrained model?  \n",
    "The vectors generated by the pretrained model are more accurate and have a higher cosine similarity than the vectors generated by myself. This is because the pretrained model has been trained on a much larger dataset than the dataset I used to train my model.  \n",
    "### Which of the Word2Vec models seems to encode semantic similarities between words better?  \n",
    "The google news model performs better in general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_word2vec(tokens, model):\n",
    "    vectors = [model[word] for word in tokens if word in model]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data['average_word2vec'] = final_data['tokens'].apply(lambda x: get_average_word2vec(x, google_w2v_model))\n",
    "\n",
    "# Split the dataset into a training and testing set\n",
    "X_w2v = np.vstack(final_data['average_word2vec'].values)\n",
    "y_w2v = final_data['target'].values\n",
    "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(X_w2v, y_w2v, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf = final_data['review_body'].values\n",
    "y_tfidf = final_data['target'].values\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(X_tfidf, y_tfidf, test_size=0.2, random_state=RANDOM_SEED)\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Perceptron (Word2Vec): 0.79855\n",
      "Accuracy for Perceptron (TF-IDF): 0.81835\n"
     ]
    }
   ],
   "source": [
    "# Train a Perceptron model\n",
    "perceptron_model = Perceptron()\n",
    "perceptron_model.fit(X_train_w2v, y_train_w2v)\n",
    "y_pred_perceptron = perceptron_model.predict(X_test_w2v)\n",
    "\n",
    "accuracy_perceptron_word2vec = accuracy_score(y_test_w2v, y_pred_perceptron)\n",
    "\n",
    "print(\"Accuracy for Perceptron (Word2Vec):\", accuracy_perceptron_word2vec)\n",
    "\n",
    "perceptron_model = Perceptron()\n",
    "perceptron_model.fit(X_train_tfidf, y_train_tfidf)\n",
    "y_pred_perceptron = perceptron_model.predict(X_test_tfidf)\n",
    "\n",
    "accuracy_perceptron_tfidf = accuracy_score(y_test_tfidf, y_pred_perceptron)\n",
    "\n",
    "print(\"Accuracy for Perceptron (TF-IDF):\", accuracy_perceptron_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jehil\\OneDrive\\Documents\\GitHub\\CSCI-544\\.conda\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for SVM (Word2Vec): 0.8184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jehil\\OneDrive\\Documents\\GitHub\\CSCI-544\\.conda\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for SVM (TF-IDF): 0.86485\n"
     ]
    }
   ],
   "source": [
    "svm_model = LinearSVC()\n",
    "svm_model.fit(X_train_w2v, y_train_w2v)\n",
    "y_pred_svm = svm_model.predict(X_test_w2v)\n",
    "\n",
    "accuracy_svm_word2vec = accuracy_score(y_test_w2v, y_pred_svm)\n",
    "\n",
    "print(\"Accuracy for SVM (Word2Vec):\", accuracy_svm_word2vec)\n",
    "\n",
    "svm_model = LinearSVC()\n",
    "svm_model.fit(X_train_tfidf, y_train_tfidf)\n",
    "y_pred_svm = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "accuracy_svm_tfidf = accuracy_score(y_test_tfidf, y_pred_svm)\n",
    "\n",
    "print(\"Accuracy for SVM (TF-IDF):\", accuracy_svm_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you conclude from comparing performances for the models trained using the two different feature types (TF-IDF and your trained 2 Word2Vec features)?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v_tensor = torch.tensor(X_train_w2v, dtype=torch.float32)\n",
    "y_train_w2v_tensor = torch.tensor(y_train_w2v, dtype=torch.long)\n",
    "\n",
    "X_test_w2v_tensor = torch.tensor(X_test_w2v, dtype=torch.float32)\n",
    "y_test_w2v_tensor = torch.tensor(y_test_w2v, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using all features in a Feedforward network: 0.742\n"
     ]
    }
   ],
   "source": [
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_layer_size):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.fc1 = nn.Linear(input_layer_size, 50)\n",
    "        self.fc2 = nn.Linear(50, 5)\n",
    "        self.fc3 = nn.Linear(5, 2) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "model = MLPModel(300)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_w2v_tensor)\n",
    "    loss = criterion(outputs, y_train_w2v_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    outputs = model(X_test_w2v_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = accuracy_score(y_test_w2v_tensor, predicted)\n",
    "    print(f\"Accuracy using all features in a Feedforward network: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_ten_word2vec(tokens, model):\n",
    "    vectors = []\n",
    "    for i in range(10):\n",
    "        if i < len(tokens) and tokens[i] in model:\n",
    "            vectors.append(model[tokens[i]])\n",
    "        else:\n",
    "            vectors.append(np.zeros(model.vector_size))\n",
    "    return np.array(vectors)\n",
    "\n",
    "def get_first_ten_flattened_word2vec(tokens, model):\n",
    "    return get_first_ten_word2vec(tokens, model).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data['first_ten_words_word2vec'] = final_data['tokens'].apply(lambda x: get_first_ten_flattened_word2vec(x, google_w2v_model))\n",
    "X_w2v_3000 = np.vstack(final_data['first_ten_words_word2vec'].values)\n",
    "X_train_w2v_3000, X_test_w2v_3000, y_train_w2v_3000, y_test_w2v_3000 = train_test_split(X_w2v_3000, y_w2v, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "X_train_w2v_3000_tensor = torch.tensor(X_train_w2v_3000, dtype=torch.float32)\n",
    "y_train_w2v_3000_tensor = torch.tensor(y_train_w2v_3000, dtype=torch.long)\n",
    "\n",
    "X_test_w2v_3000_tensor = torch.tensor(X_test_w2v_3000, dtype=torch.float32)\n",
    "y_test_w2v_3000_tensor = torch.tensor(y_test_w2v_3000, dtype=torch.long)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using all features in a Feedforward network: 0.6453\n"
     ]
    }
   ],
   "source": [
    "model = MLPModel(3000)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_w2v_3000_tensor)\n",
    "    loss = criterion(outputs, y_train_w2v_3000_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    outputs = model(X_test_w2v_3000_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = accuracy_score(y_test_w2v_3000_tensor, predicted)\n",
    "    print(f\"Accuracy using all features in a Feedforward network: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you conclude by comparing accuracy values you obtain with those obtained in the “’Simple Models” section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v_rnn_tensor = X_train_w2v_3000_tensor.reshape(80000, 10, 300)\n",
    "X_test_w2v_rnn_tensor = X_test_w2v_3000_tensor.reshape(20000, 10, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6403\n"
     ]
    }
   ],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, rnn_type=\"rnn\"):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        if rnn_type == \"rnn\":\n",
    "            self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        elif rnn_type == \"lstm\":\n",
    "            self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        else:\n",
    "            self.rnn = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        \n",
    "        if isinstance(self.rnn, nn.LSTM):\n",
    "            c0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "            out, _ = self.rnn(x, (h0, c0))\n",
    "        else:\n",
    "            out, _ = self.rnn(x, h0)\n",
    "        \n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7577\n"
     ]
    }
   ],
   "source": [
    "input_size = 300\n",
    "hidden_size = 10\n",
    "num_classes = 2\n",
    "rnn = RNNModel(input_size, hidden_size, num_classes, rnn_type=\"rnn\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = rnn(X_train_w2v_rnn_tensor)\n",
    "    loss = criterion(outputs, y_train_w2v_3000_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "with torch.no_grad():\n",
    "    rnn.eval()\n",
    "    outputs = rnn(X_test_w2v_rnn_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = accuracy_score(y_test_w2v_3000_tensor, predicted)\n",
    "    print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77025\n"
     ]
    }
   ],
   "source": [
    "input_size = 300\n",
    "hidden_size = 10\n",
    "num_classes = 2\n",
    "rnn = RNNModel(input_size, hidden_size, num_classes, rnn_type=\"gru\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = rnn(X_train_w2v_rnn_tensor)\n",
    "    loss = criterion(outputs, y_train_w2v_3000_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "with torch.no_grad():\n",
    "    rnn.eval()\n",
    "    outputs = rnn(X_test_w2v_rnn_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = accuracy_score(y_test_w2v_3000_tensor, predicted)\n",
    "    print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Epoch: 11\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20\n",
      "Epoch: 21\n",
      "Epoch: 22\n",
      "Epoch: 23\n",
      "Epoch: 24\n",
      "Epoch: 25\n",
      "Epoch: 26\n",
      "Epoch: 27\n",
      "Epoch: 28\n",
      "Epoch: 29\n",
      "Epoch: 30\n",
      "Epoch: 31\n",
      "Epoch: 32\n",
      "Epoch: 33\n",
      "Epoch: 34\n",
      "Epoch: 35\n",
      "Epoch: 36\n",
      "Epoch: 37\n",
      "Epoch: 38\n",
      "Epoch: 39\n",
      "Epoch: 40\n",
      "Epoch: 41\n",
      "Epoch: 42\n",
      "Epoch: 43\n",
      "Epoch: 44\n",
      "Epoch: 45\n",
      "Epoch: 46\n",
      "Epoch: 47\n",
      "Epoch: 48\n",
      "Epoch: 49\n",
      "Epoch: 50\n",
      "Epoch: 51\n",
      "Epoch: 52\n",
      "Epoch: 53\n",
      "Epoch: 54\n",
      "Epoch: 55\n",
      "Epoch: 56\n",
      "Epoch: 57\n",
      "Epoch: 58\n",
      "Epoch: 59\n",
      "Epoch: 60\n",
      "Epoch: 61\n",
      "Epoch: 62\n",
      "Epoch: 63\n",
      "Epoch: 64\n",
      "Epoch: 65\n",
      "Epoch: 66\n",
      "Epoch: 67\n",
      "Epoch: 68\n",
      "Epoch: 69\n",
      "Epoch: 70\n",
      "Epoch: 71\n",
      "Epoch: 72\n",
      "Epoch: 73\n",
      "Epoch: 74\n",
      "Epoch: 75\n",
      "Epoch: 76\n",
      "Epoch: 77\n",
      "Epoch: 78\n",
      "Epoch: 79\n",
      "Epoch: 80\n",
      "Epoch: 81\n",
      "Epoch: 82\n",
      "Epoch: 83\n",
      "Epoch: 84\n",
      "Epoch: 85\n",
      "Epoch: 86\n",
      "Epoch: 87\n",
      "Epoch: 88\n",
      "Epoch: 89\n",
      "Epoch: 90\n",
      "Epoch: 91\n",
      "Epoch: 92\n",
      "Epoch: 93\n",
      "Epoch: 94\n",
      "Epoch: 95\n",
      "Epoch: 96\n",
      "Epoch: 97\n",
      "Epoch: 98\n",
      "Epoch: 99\n",
      "Epoch: 100\n",
      "Epoch: 101\n",
      "Epoch: 102\n",
      "Epoch: 103\n",
      "Epoch: 104\n",
      "Epoch: 105\n",
      "Epoch: 106\n",
      "Epoch: 107\n",
      "Epoch: 108\n",
      "Epoch: 109\n",
      "Epoch: 110\n",
      "Epoch: 111\n",
      "Epoch: 112\n",
      "Epoch: 113\n",
      "Epoch: 114\n",
      "Epoch: 115\n",
      "Epoch: 116\n",
      "Epoch: 117\n",
      "Epoch: 118\n",
      "Epoch: 119\n",
      "Epoch: 120\n",
      "Epoch: 121\n",
      "Epoch: 122\n",
      "Epoch: 123\n",
      "Epoch: 124\n",
      "Epoch: 125\n",
      "Epoch: 126\n",
      "Epoch: 127\n",
      "Epoch: 128\n",
      "Epoch: 129\n",
      "Epoch: 130\n",
      "Epoch: 131\n",
      "Epoch: 132\n",
      "Epoch: 133\n",
      "Epoch: 134\n",
      "Epoch: 135\n",
      "Epoch: 136\n",
      "Epoch: 137\n",
      "Epoch: 138\n",
      "Epoch: 139\n",
      "Epoch: 140\n",
      "Epoch: 141\n",
      "Epoch: 142\n",
      "Epoch: 143\n",
      "Epoch: 144\n",
      "Epoch: 145\n",
      "Epoch: 146\n",
      "Epoch: 147\n",
      "Epoch: 148\n",
      "Epoch: 149\n",
      "Epoch: 150\n",
      "Epoch: 151\n",
      "Epoch: 152\n",
      "Epoch: 153\n",
      "Epoch: 154\n",
      "Epoch: 155\n",
      "Epoch: 156\n",
      "Epoch: 157\n",
      "Epoch: 158\n",
      "Epoch: 159\n",
      "Epoch: 160\n",
      "Epoch: 161\n",
      "Epoch: 162\n",
      "Epoch: 163\n",
      "Epoch: 164\n",
      "Epoch: 165\n",
      "Epoch: 166\n",
      "Epoch: 167\n",
      "Epoch: 168\n",
      "Epoch: 169\n",
      "Epoch: 170\n",
      "Epoch: 171\n",
      "Epoch: 172\n",
      "Epoch: 173\n",
      "Epoch: 174\n",
      "Epoch: 175\n",
      "Epoch: 176\n",
      "Epoch: 177\n",
      "Epoch: 178\n",
      "Epoch: 179\n",
      "Epoch: 180\n",
      "Epoch: 181\n",
      "Epoch: 182\n",
      "Epoch: 183\n",
      "Epoch: 184\n",
      "Epoch: 185\n",
      "Epoch: 186\n",
      "Epoch: 187\n",
      "Epoch: 188\n",
      "Epoch: 189\n",
      "Epoch: 190\n",
      "Epoch: 191\n",
      "Epoch: 192\n",
      "Epoch: 193\n",
      "Epoch: 194\n",
      "Epoch: 195\n",
      "Epoch: 196\n",
      "Epoch: 197\n",
      "Epoch: 198\n",
      "Epoch: 199\n",
      "Accuracy: 0.7695\n"
     ]
    }
   ],
   "source": [
    "input_size = 300\n",
    "hidden_size = 10\n",
    "num_classes = 2\n",
    "rnn = RNNModel(input_size, hidden_size, num_classes, rnn_type=\"lstm\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = rnn(X_train_w2v_rnn_tensor)\n",
    "    loss = criterion(outputs, y_train_w2v_3000_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "with torch.no_grad():\n",
    "    rnn.eval()\n",
    "    outputs = rnn(X_test_w2v_rnn_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = accuracy_score(y_test_w2v_3000_tensor, predicted)\n",
    "    print(f\"Accuracy: {accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
