{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 544 HW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Vocabulary Creation (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "TRAIN_DATA_PATH = 'data/train.json'\n",
    "DEV_DATA_PATH = 'data/dev.json'\n",
    "UNKNOWN_KEY = '<unk>'\n",
    "THRESHOLD = 2\n",
    "OUTPUT_FOLDER = 'verification/out'\n",
    "OUTPUT_PATH_VOCAB = OUTPUT_FOLDER + '/vocab.txt'\n",
    "OUTPUT_PATH_HMM = OUTPUT_FOLDER + '/hmm.json'\n",
    "OUTPUT_PATH_GREEDY = OUTPUT_FOLDER + '/greedy.json'\n",
    "OUTPUT_PATH_VITERBI = OUTPUT_FOLDER + '/viterbi.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_counts = {}\n",
    "transition_counts = defaultdict(lambda: defaultdict(int))\n",
    "emission_counts = defaultdict(lambda: defaultdict(int))\n",
    "with open(TRAIN_DATA_PATH) as f:\n",
    "    train_data = json.load(f)\n",
    "    train_data_words = []\n",
    "    for train_entry in train_data:\n",
    "        train_data_words.extend(train_entry['sentence'])\n",
    "\n",
    "temp_dict = Counter(train_data_words)\n",
    "\n",
    "freq_dict = {}\n",
    "\n",
    "freq_dict[UNKNOWN_KEY] = 0\n",
    "for word in temp_dict:\n",
    "    if temp_dict[word] < THRESHOLD:\n",
    "        freq_dict[UNKNOWN_KEY] += temp_dict[word]\n",
    "    else:\n",
    "        freq_dict[word] = temp_dict[word]\n",
    "\n",
    "unk_value = freq_dict[UNKNOWN_KEY]\n",
    "del freq_dict[UNKNOWN_KEY]\n",
    "freq_dict = dict([(UNKNOWN_KEY, unk_value)] + sorted(freq_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "with open(OUTPUT_PATH_VOCAB, 'w') as f:\n",
    "    for o, word in enumerate(freq_dict):\n",
    "        freq_dict[word] = {\n",
    "            'index': o,\n",
    "            'frequency': freq_dict[word]\n",
    "        }\n",
    "        f.write(f'{word}\\t{o}\\t{freq_dict[word][\"frequency\"]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23183, 20011)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(freq_dict), freq_dict['<unk>']['frequency']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What threshold value did you choose for identifying unknown words for replacement?\n",
    "3\n",
    "\n",
    "What is the overall size of your vocabulary, and how many times does the special token ”< unk >” occur following the replacement process?\n",
    "Vocabulary size:  16920\n",
    "< unk > count:  32357\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Model Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = {}\n",
    "for train_entry in train_data:\n",
    "    labels = train_entry['labels']\n",
    "    label_len = len(labels)\n",
    "    for s in range(label_len):\n",
    "        tag = labels[s]\n",
    "        if tag not in tags:\n",
    "            tags[tag] = {\n",
    "                'index': len(tags),\n",
    "                'frequency': 1\n",
    "            }\n",
    "        else:\n",
    "            tags[tag]['frequency'] += 1\n",
    "        if s == 0:\n",
    "            initial_counts[tag] = initial_counts.get(tag, 0) + 1\n",
    "        emitted_word = train_entry['sentence'][s] if train_entry['sentence'][s] in freq_dict else UNKNOWN_KEY\n",
    "        emission_counts[tag][emitted_word] += 1\n",
    "        if s < label_len - 1:\n",
    "            next_tag = labels[s + 1]\n",
    "            transition_counts[tag][next_tag] += 1\n",
    "\n",
    "NUM_TAGS = len(tags)\n",
    "NUM_WORDS = len(freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9987101634553922"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(transition_counts['NNP'].values()) / tags['NNP']['frequency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition = {}\n",
    "emission = {}\n",
    "\n",
    "for tag in transition_counts:\n",
    "    for next_tag in transition_counts[tag]:\n",
    "        transition[f'({tag},{next_tag})'] = transition_counts[tag][next_tag] / tags[tag]['frequency']\n",
    "\n",
    "for tag in emission_counts:\n",
    "    for next_tag in emission_counts[tag]:\n",
    "        emission[f'({tag},{next_tag})'] = emission_counts[tag][next_tag] / tags[tag]['frequency']\n",
    "\n",
    "hmm_json = {\n",
    "    'transition': transition,\n",
    "    'emission': emission,\n",
    "}\n",
    "\n",
    "with open(OUTPUT_PATH_HMM, 'w') as json_file:\n",
    "    json.dump(hmm_json, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1351, 30303)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(transition), len(emission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many transition and emission parameters in your HMM?\n",
    "\n",
    "Transition parameters:  1351\n",
    "Emission parameters:  23373"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Greedy Decoding with HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_prob = np.zeros(NUM_TAGS)\n",
    "for o, tag in enumerate(tags):\n",
    "    initial_prob[o] = initial_counts.get(tag, 0) / len(train_data)\n",
    "\n",
    "transition_prob = np.zeros((NUM_TAGS, NUM_TAGS))\n",
    "for tag in transition_counts:\n",
    "    for next_tag in transition_counts[tag]:\n",
    "        transition_prob[tags[tag]['index']][tags[next_tag]['index']] = transition_counts[tag][next_tag] / tags[tag]['frequency']\n",
    "\n",
    "emission_prob = np.zeros((NUM_TAGS, NUM_WORDS))\n",
    "for tag in emission_counts:\n",
    "    for word in emission_counts[tag]:\n",
    "        emission_prob[tags[tag]['index']][freq_dict[word]['index']] = emission_counts[tag][word] / tags[tag]['frequency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9350297492562686\n"
     ]
    }
   ],
   "source": [
    "with open(DEV_DATA_PATH) as f:\n",
    "    dev_data = json.load(f)\n",
    "\n",
    "greedy = []\n",
    "tag_list = list(tags.keys())\n",
    "res = np.array([], dtype=bool)\n",
    "\n",
    "for data_idx, dev_entry in enumerate(dev_data):\n",
    "    sentence = dev_entry['sentence']\n",
    "    pred = []\n",
    "    for o, word in enumerate(sentence):\n",
    "        init_prob = initial_prob if o == 0 else transition_prob[tags[pred[-1]]['index']]\n",
    "        mul_value = init_prob * emission_prob[:, freq_dict.get(word, freq_dict[UNKNOWN_KEY])['index']]\n",
    "        pred.append(tag_list[np.argmax(mul_value)])\n",
    "    greedy.append({\n",
    "        'index': data_idx,\n",
    "        'sentence': sentence,\n",
    "        'labels': pred\n",
    "        })\n",
    "    res = np.append(res, np.array(pred) == np.array(dev_entry['labels']))\n",
    "print(res.mean())\n",
    "with open(OUTPUT_PATH_GREEDY, 'w') as json_file:\n",
    "    json.dump(greedy, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the accuracy on the dev data? 0.9298615748891992"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Viterbi Decoding with HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(mu_prev: np.ndarray,\n",
    "         emission_probs: np.ndarray,\n",
    "         transition_probs: np.ndarray,\n",
    "         observed_state: int):\n",
    "    pre_max = mu_prev * transition_probs.T\n",
    "    max_prev_states = np.argmax(pre_max, axis=1)\n",
    "    max_vals = pre_max[np.arange(len(max_prev_states)), max_prev_states]\n",
    "    mu_new = max_vals * emission_probs[:, observed_state]\n",
    "    return mu_new, max_prev_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9476883613623945\n"
     ]
    }
   ],
   "source": [
    "def viterbi(sentence, initial_prob, transition_prob, emission_prob):\n",
    "\n",
    "    text_length = len(sentence)\n",
    "    viterbi_matrix = np.zeros((NUM_TAGS, text_length))\n",
    "    backpointers = np.zeros((NUM_TAGS, text_length), dtype=int)\n",
    "\n",
    "    word = sentence[0] if sentence[0] in freq_dict else UNKNOWN_KEY\n",
    "    word_index = freq_dict[word]['index']\n",
    "    viterbi_matrix[:, 0] = initial_prob * emission_prob[:, word_index]\n",
    "\n",
    "    for t in range(1, len(sentence)):\n",
    "        for i, tag in enumerate(tags):\n",
    "            word = sentence[t] if sentence[t] in freq_dict else UNKNOWN_KEY\n",
    "            word_index = freq_dict[word]['index']\n",
    "\n",
    "            max_prob = 0\n",
    "            max_index = 0\n",
    "            for j, prev_tag in enumerate(tags):\n",
    "                transition = transition_prob[j, i]\n",
    "                emission = emission_prob[i, word_index]\n",
    "                prob = viterbi_matrix[j, t - 1] * transition * emission\n",
    "\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    max_index = j\n",
    "\n",
    "            viterbi_matrix[i, t] = max_prob\n",
    "            backpointers[i, t] = max_index\n",
    "\n",
    "    # Backtrack to find the best sequence of tags\n",
    "    best_sequence = []\n",
    "    max_prob = 0\n",
    "    max_index = 0\n",
    "    for i in range(NUM_TAGS):\n",
    "        if viterbi_matrix[i, len(sentence) - 1] > max_prob:\n",
    "            max_prob = viterbi_matrix[i, len(sentence) - 1]\n",
    "            max_index = i\n",
    "\n",
    "    best_sequence.append(max_index)\n",
    "    for t in range(len(sentence) - 1, 0, -1):\n",
    "        max_index = backpointers[max_index, t]\n",
    "        best_sequence.append(max_index)\n",
    "\n",
    "    best_sequence = best_sequence[::-1]  # Reverse the sequence\n",
    "\n",
    "    return [tag_list[i] for i in best_sequence]\n",
    "\n",
    "def calculate_accuracy(predictions, labels):\n",
    "    correct = sum(1 for p, l in zip(predictions, labels) if p == l)\n",
    "    total = len(predictions)\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "with open(DEV_DATA_PATH) as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Perform POS tagging using Viterbi and calculate accuracy\n",
    "predictions = []\n",
    "true_labels = []\n",
    "for test_entry in test_data:\n",
    "    sentence = test_entry['sentence']\n",
    "    true_label = test_entry['labels']\n",
    "    predicted_label = viterbi(sentence, initial_prob, transition_prob, emission_prob)\n",
    "    predictions.extend(predicted_label)\n",
    "    true_labels.extend(true_label)\n",
    "\n",
    "accuracy = (np.array(predictions) == np.array(true_labels)).mean()\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jehil\\AppData\\Local\\Temp\\ipykernel_24228\\3903555661.py:13: RuntimeWarning: divide by zero encountered in log\n",
      "  initial_prob_matrix = np.log(initial_prob_matrix)\n",
      "C:\\Users\\jehil\\AppData\\Local\\Temp\\ipykernel_24228\\3903555661.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  emission_prob_matrix = np.log(emission_prob[:, word_indices])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (45,37) (45,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jehil\\OneDrive\\Documents\\GitHub\\CSCI-544\\HW2\\HW2.ipynb Cell 21\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jehil/OneDrive/Documents/GitHub/CSCI-544/HW2/HW2.ipynb#X44sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m sentence \u001b[39m=\u001b[39m test_entry[\u001b[39m'\u001b[39m\u001b[39msentence\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jehil/OneDrive/Documents/GitHub/CSCI-544/HW2/HW2.ipynb#X44sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m true_label \u001b[39m=\u001b[39m test_entry[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jehil/OneDrive/Documents/GitHub/CSCI-544/HW2/HW2.ipynb#X44sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m predicted_label \u001b[39m=\u001b[39m viterbi_vectorized(sentence, initial_prob, transition_prob, emission_prob, tags, freq_dict)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jehil/OneDrive/Documents/GitHub/CSCI-544/HW2/HW2.ipynb#X44sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m predictions\u001b[39m.\u001b[39mextend(predicted_label)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jehil/OneDrive/Documents/GitHub/CSCI-544/HW2/HW2.ipynb#X44sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m true_labels\u001b[39m.\u001b[39mextend(true_label)\n",
      "\u001b[1;32mc:\\Users\\jehil\\OneDrive\\Documents\\GitHub\\CSCI-544\\HW2\\HW2.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jehil/OneDrive/Documents/GitHub/CSCI-544/HW2/HW2.ipynb#X44sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m initial_prob_matrix \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlog(initial_prob_matrix)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jehil/OneDrive/Documents/GitHub/CSCI-544/HW2/HW2.ipynb#X44sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m emission_prob_matrix \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlog(emission_prob[:, word_indices])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jehil/OneDrive/Documents/GitHub/CSCI-544/HW2/HW2.ipynb#X44sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m viterbi_matrix[:, \u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m initial_prob_matrix \u001b[39m+\u001b[39;49m emission_prob_matrix[:, \u001b[39m0\u001b[39;49m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jehil/OneDrive/Documents/GitHub/CSCI-544/HW2/HW2.ipynb#X44sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Fill in the rest of the Viterbi matrix\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jehil/OneDrive/Documents/GitHub/CSCI-544/HW2/HW2.ipynb#X44sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, sentence_length):\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (45,37) (45,) "
     ]
    }
   ],
   "source": [
    "def viterbi_vectorized(sentence, initial_prob, transition_prob, emission_prob, tags, freq_dict):\n",
    "    num_tags = len(tags)\n",
    "    num_words = len(freq_dict)\n",
    "    sentence_length = len(sentence)\n",
    "\n",
    "    # Create matrices for the Viterbi algorithm\n",
    "    viterbi_matrix = np.zeros((num_tags, sentence_length))\n",
    "    backpointers = np.zeros((num_tags, sentence_length), dtype=int)\n",
    "\n",
    "    # Initialize the first column of the Viterbi matrix\n",
    "    word_indices = [freq_dict.get(word, freq_dict[UNKNOWN_KEY])['index'] for word in sentence]\n",
    "    # initial_prob_matrix = np.tile(initial_prob, (sentence_length, 1)).T\n",
    "    # initial_prob_matrix = np.log(initial_prob_matrix)\n",
    "    emission_prob_matrix = np.log(emission_prob[:, word_indices])\n",
    "\n",
    "    viterbi_matrix[:, 0] = initial_prob + emission_prob_matrix[:, 0]\n",
    "\n",
    "    # Fill in the rest of the Viterbi matrix\n",
    "    for t in range(1, sentence_length):\n",
    "        transition_prob_matrix = np.log(transition_prob)\n",
    "        scores = viterbi_matrix[:, t - 1] + transition_prob_matrix.T + emission_prob_matrix[:, t]\n",
    "        backpointers[:, t] = np.argmax(scores, axis=0)\n",
    "        viterbi_matrix[:, t] = np.max(scores, axis=0)\n",
    "\n",
    "    # Backtrack to find the best sequence of tags\n",
    "    best_sequence = [np.argmax(viterbi_matrix[:, -1])]\n",
    "    for t in range(sentence_length - 1, 0, -1):\n",
    "        best_sequence.append(backpointers[best_sequence[-1], t])\n",
    "\n",
    "    best_sequence = best_sequence[::-1]  # Reverse the sequence\n",
    "\n",
    "    return [tag_list[i] for i in best_sequence]\n",
    "\n",
    "# Perform POS tagging using Viterbi and calculate accuracy\n",
    "predictions = []\n",
    "true_labels = []\n",
    "for test_entry in dev_data:\n",
    "    sentence = test_entry['sentence']\n",
    "    true_label = test_entry['labels']\n",
    "    predicted_label = viterbi_vectorized(sentence, initial_prob, transition_prob, emission_prob, tags, freq_dict)\n",
    "    predictions.extend(predicted_label)\n",
    "    true_labels.extend(true_label)\n",
    "\n",
    "accuracy = (np.array(predictions) == np.array(true_labels)).mean()\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
