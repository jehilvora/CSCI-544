{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 544 HW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Vocabulary Creation (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "TRAIN_DATA_PATH = 'data/train.json'\n",
    "DEV_DATA_PATH = 'data/dev.json'\n",
    "UNKNOWN_KEY = '<unk>'\n",
    "THRESHOLD = 3\n",
    "OUTPUT_FOLDER = 'verification/out'\n",
    "OUTPUT_PATH_VOCAB = OUTPUT_FOLDER + '/vocab.txt'\n",
    "OUTPUT_PATH_HMM = OUTPUT_FOLDER + '/hmm.json'\n",
    "OUTPUT_PATH_GREEDY = OUTPUT_FOLDER + '/greedy.json'\n",
    "OUTPUT_PATH_VITERBI = OUTPUT_FOLDER + '/viterbi.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_counts = {}\n",
    "transition_counts = defaultdict(lambda: defaultdict(int))\n",
    "emission_counts = defaultdict(lambda: defaultdict(int))\n",
    "with open(TRAIN_DATA_PATH) as f:\n",
    "    train_data = json.load(f)\n",
    "    train_data_words = []\n",
    "    for train_entry in train_data:\n",
    "        train_data_words.extend(train_entry['sentence'])\n",
    "\n",
    "temp_dict = Counter(train_data_words)\n",
    "\n",
    "freq_dict = {}\n",
    "\n",
    "freq_dict[UNKNOWN_KEY] = 0\n",
    "for word in temp_dict:\n",
    "    if temp_dict[word] < THRESHOLD:\n",
    "        freq_dict[UNKNOWN_KEY] += temp_dict[word]\n",
    "    else:\n",
    "        freq_dict[word] = temp_dict[word]\n",
    "\n",
    "unk_value = freq_dict[UNKNOWN_KEY]\n",
    "del freq_dict[UNKNOWN_KEY]\n",
    "freq_dict = dict([(UNKNOWN_KEY, unk_value)] + sorted(freq_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "with open(OUTPUT_PATH_VOCAB, 'w') as f:\n",
    "    for o, word in enumerate(freq_dict):\n",
    "        freq_dict[word] = {\n",
    "            'index': o,\n",
    "            'frequency': freq_dict[word]\n",
    "        }\n",
    "        f.write(f'{word}\\t{o}\\t{freq_dict[word][\"frequency\"]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What threshold value did you choose for identifying unknown words for replacement?\n",
    "3\n",
    "\n",
    "What is the overall size of your vocabulary, and how many times does the special token ”< unk >” occur following the replacement process?\n",
    "Vocabulary size:  16920\n",
    "< unk > count:  32357\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Model Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = {}\n",
    "for train_entry in train_data:\n",
    "    labels = train_entry['labels']\n",
    "    label_len = len(labels)\n",
    "    for s in range(label_len):\n",
    "        tag = labels[s]\n",
    "        if tag not in tags:\n",
    "            tags[tag] = {\n",
    "                'index': len(tags),\n",
    "                'frequency': 1\n",
    "            }\n",
    "        else:\n",
    "            tags[tag]['frequency'] += 1\n",
    "        if s == 0:\n",
    "            initial_counts[tag] = initial_counts.get(tag, 0) + 1\n",
    "        emitted_word = train_entry['sentence'][s] if train_entry['sentence'][s] in freq_dict else UNKNOWN_KEY\n",
    "        emission_counts[tag][emitted_word] += 1\n",
    "        if s < label_len - 1:\n",
    "            next_tag = labels[s + 1]\n",
    "            transition_counts[tag][next_tag] += 1\n",
    "\n",
    "NUM_TAGS = len(tags)\n",
    "NUM_WORDS = len(freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition = {}\n",
    "emission = {}\n",
    "\n",
    "for tag in transition_counts:\n",
    "    for next_tag in transition_counts[tag]:\n",
    "        transition[f'({tag},{next_tag})'] = transition_counts[tag][next_tag] / tags[tag]['frequency']\n",
    "\n",
    "for tag in emission_counts:\n",
    "    for next_tag in emission_counts[tag]:\n",
    "        emission[f'({tag},{next_tag})'] = emission_counts[tag][next_tag] / tags[tag]['frequency']\n",
    "\n",
    "hmm_json = {\n",
    "    'transition': transition,\n",
    "    'emission': emission,\n",
    "}\n",
    "\n",
    "with open(OUTPUT_PATH_HMM, 'w') as json_file:\n",
    "    json.dump(hmm_json, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1351, 23373)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(transition), len(emission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many transition and emission parameters in your HMM?\n",
    "\n",
    "Transition parameters:  1351\n",
    "Emission parameters:  23373"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Greedy Decoding with HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_prob = np.zeros(NUM_TAGS)\n",
    "for o, tag in enumerate(tags):\n",
    "    initial_prob[o] = initial_counts.get(tag, 0) / len(train_data)\n",
    "\n",
    "transition_prob = np.zeros((NUM_TAGS, NUM_TAGS))\n",
    "for tag in transition_counts:\n",
    "    for next_tag in transition_counts[tag]:\n",
    "        transition_prob[tags[tag]['index']][tags[next_tag]['index']] = transition_counts[tag][next_tag] / tags[tag]['frequency']\n",
    "\n",
    "emission_prob = np.zeros((NUM_WORDS, NUM_TAGS))\n",
    "for tag in emission_counts:\n",
    "    for word in emission_counts[tag]:\n",
    "        emission_prob[freq_dict[word]['index']][tags[tag]['index']] = emission_counts[tag][word] / tags[tag]['frequency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9298615748891992\n"
     ]
    }
   ],
   "source": [
    "with open(DEV_DATA_PATH) as f:\n",
    "    dev_data = json.load(f)\n",
    "\n",
    "greedy = []\n",
    "tag_list = list(tags.keys())\n",
    "res = np.array([], dtype=bool)\n",
    "\n",
    "for data_idx, dev_entry in enumerate(dev_data):\n",
    "    sentence = dev_entry['sentence']\n",
    "    pred = []\n",
    "    for o, word in enumerate(sentence):\n",
    "        init_prob = initial_prob if o == 0 else transition_prob[tags[pred[-1]]['index']]\n",
    "        mul_value = init_prob * emission_prob[freq_dict.get(word, freq_dict[UNKNOWN_KEY])['index']]\n",
    "        pred.append(tag_list[np.argmax(mul_value)])\n",
    "    greedy.append({\n",
    "        'index': data_idx,\n",
    "        'sentence': sentence,\n",
    "        'labels': pred\n",
    "        })\n",
    "    res = np.append(res, np.array(pred) == np.array(dev_entry['labels']))\n",
    "print(res.mean())\n",
    "with open(OUTPUT_PATH_GREEDY, 'w') as json_file:\n",
    "    json.dump(greedy, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the accuracy on the dev data? 0.9298615748891992"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Viterbi Decoding with HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not numpy.float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jehil\\OneDrive\\Documents\\GitHub\\CSCI-544\\HW2\\HW2.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jehil/OneDrive/Documents/GitHub/CSCI-544/HW2/HW2.ipynb#X30sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m k \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39margmax(trellis[:, T \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jehil/OneDrive/Documents/GitHub/CSCI-544/HW2/HW2.ipynb#X30sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m o \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(T \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jehil/OneDrive/Documents/GitHub/CSCI-544/HW2/HW2.ipynb#X30sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     pred\u001b[39m.\u001b[39minsert(\u001b[39m0\u001b[39m, tag_list[k])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jehil/OneDrive/Documents/GitHub/CSCI-544/HW2/HW2.ipynb#X30sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     k \u001b[39m=\u001b[39m pointers[k, o]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jehil/OneDrive/Documents/GitHub/CSCI-544/HW2/HW2.ipynb#X30sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m viterbi\u001b[39m.\u001b[39mappend({\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jehil/OneDrive/Documents/GitHub/CSCI-544/HW2/HW2.ipynb#X30sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m: data_idx,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jehil/OneDrive/Documents/GitHub/CSCI-544/HW2/HW2.ipynb#X30sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39msentence\u001b[39m\u001b[39m'\u001b[39m: sentence,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jehil/OneDrive/Documents/GitHub/CSCI-544/HW2/HW2.ipynb#X30sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m: pred\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jehil/OneDrive/Documents/GitHub/CSCI-544/HW2/HW2.ipynb#X30sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     })\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not numpy.float64"
     ]
    }
   ],
   "source": [
    "viterbi = []\n",
    "res = np.array([], dtype=bool)\n",
    "\n",
    "for data_idx, dev_entry in enumerate(dev_data):\n",
    "    sentence = dev_entry['sentence']\n",
    "    T = len(sentence)\n",
    "    trellis = np.zeros((NUM_TAGS, T))\n",
    "    pointers = np.zeros((NUM_TAGS, T))\n",
    "    trellis[: , 0] = initial_prob * emission_prob[freq_dict.get(sentence[0], freq_dict[UNKNOWN_KEY])['index']]\n",
    "    for o in range(1, T):\n",
    "        for s in range(NUM_TAGS):\n",
    "            k = np.argmax(trellis[k, o - 1] * transition_prob[k, s] * emission_prob[o, s] for k in range(NUM_TAGS))\n",
    "            trellis[s, o] = trellis[k, o - 1] * transition_prob[k, s] * emission_prob[o, s]\n",
    "            pointers[s, o] = k\n",
    "    pred = []\n",
    "    k = np.argmax(trellis[:, T - 1])\n",
    "    for o in range(T - 1, -1, -1):\n",
    "        pred.insert(0, tag_list[int(k)])\n",
    "        k = pointers[k, o]\n",
    "    viterbi.append({\n",
    "        'index': data_idx,\n",
    "        'sentence': sentence,\n",
    "        'labels': pred\n",
    "        })\n",
    "    print(\"Prediction:\", pred)\n",
    "    print(\"Actual:\", dev_entry['labels'])\n",
    "    res = np.append(res, np.array(pred) == np.array(dev_entry['labels']))\n",
    "print(res.mean())\n",
    "with open(OUTPUT_PATH_VITERBI, 'w') as json_file:\n",
    "    json.dump(viterbi, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01827564821560826"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trellis[:, 0].sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
