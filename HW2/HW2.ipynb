{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 544 HW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Vocabulary Creation (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "TRAIN_DATA_PATH = 'data/train.json'\n",
    "DEV_DATA_PATH = 'data/dev.json'\n",
    "TEST_DATA_PATH = 'data/test.json'\n",
    "UNKNOWN_KEY = '<unk>'\n",
    "THRESHOLD = 2\n",
    "OUTPUT_FOLDER = 'verification/out'\n",
    "OUTPUT_PATH_VOCAB = OUTPUT_FOLDER + '/vocab.txt'\n",
    "OUTPUT_PATH_HMM = OUTPUT_FOLDER + '/hmm.json'\n",
    "OUTPUT_PATH_GREEDY = OUTPUT_FOLDER + '/greedy.json'\n",
    "OUTPUT_PATH_VITERBI = OUTPUT_FOLDER + '/viterbi.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct counts for the words in the training data. Replace words lower than the threshold into \\<unk\\> and create the vocabulary file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_counts = {}\n",
    "transition_counts = defaultdict(lambda: defaultdict(int))\n",
    "emission_counts = defaultdict(lambda: defaultdict(int))\n",
    "with open(TRAIN_DATA_PATH) as f:\n",
    "    train_data = json.load(f)\n",
    "    train_data_words = []\n",
    "    for train_entry in train_data:\n",
    "        train_data_words.extend(train_entry['sentence'])\n",
    "\n",
    "temp_dict = Counter(train_data_words)\n",
    "\n",
    "freq_dict = {}\n",
    "\n",
    "freq_dict[UNKNOWN_KEY] = 0\n",
    "for word in temp_dict:\n",
    "    if temp_dict[word] < THRESHOLD:\n",
    "        freq_dict[UNKNOWN_KEY] += temp_dict[word]\n",
    "    else:\n",
    "        freq_dict[word] = temp_dict[word]\n",
    "\n",
    "unk_value = freq_dict[UNKNOWN_KEY]\n",
    "del freq_dict[UNKNOWN_KEY]\n",
    "freq_dict = dict([(UNKNOWN_KEY, unk_value)] + sorted(freq_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "with open(OUTPUT_PATH_VOCAB, 'w') as f:\n",
    "    for o, word in enumerate(freq_dict):\n",
    "        freq_dict[word] = {\n",
    "            'index': o,\n",
    "            'frequency': freq_dict[word]\n",
    "        }\n",
    "        f.write(f'{word}\\t{o}\\t{freq_dict[word][\"frequency\"]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What threshold value did you choose for identifying unknown words for replacement? : 2\n",
      "What is the overall size of your vocabulary? : 23183\n",
      "How many times does the special token '< unk >' occur following the replacement process? : 20011\n"
     ]
    }
   ],
   "source": [
    "print(f\"What threshold value did you choose for identifying unknown words for replacement? : {THRESHOLD}\")\n",
    "print(f\"What is the overall size of your vocabulary? : {len(freq_dict)}\")\n",
    "print(f\"How many times does the special token '< unk >' occur following the replacement process? : {freq_dict['<unk>']['frequency']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What threshold value did you choose for identifying unknown words for replacement? : 2\n",
    "What is the overall size of your vocabulary? : 23183\n",
    "How many times does the special token '< unk >' occur following the replacement process? : 20011"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What threshold value did you choose for identifying unknown words for replacement?\n",
    "2\n",
    "\n",
    "What is the overall size of your vocabulary, and how many times does the special token \"< unk >\" occur following the replacement process?\n",
    "Vocabulary size:  23183\n",
    "< unk > count:  20011\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Model Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count transition and emission and tag frequencies from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = {}\n",
    "for train_entry in train_data:\n",
    "    labels = train_entry['labels']\n",
    "    label_len = len(labels)\n",
    "    for s in range(label_len):\n",
    "        tag = labels[s]\n",
    "        if tag not in tags:\n",
    "            tags[tag] = {\n",
    "                'index': len(tags),\n",
    "                'frequency': 1\n",
    "            }\n",
    "        else:\n",
    "            tags[tag]['frequency'] += 1\n",
    "        if s == 0:\n",
    "            initial_counts[tag] = initial_counts.get(tag, 0) + 1\n",
    "        emitted_word = train_entry['sentence'][s] if train_entry['sentence'][s] in freq_dict else UNKNOWN_KEY\n",
    "        emission_counts[tag][emitted_word] += 1\n",
    "        if s < label_len - 1:\n",
    "            next_tag = labels[s + 1]\n",
    "            transition_counts[tag][next_tag] += 1\n",
    "            tags[tag]['trans_freq'] = tags[tag].get('trans_freq', 0) + 1\n",
    "            \n",
    "\n",
    "NUM_TAGS = len(tags)\n",
    "NUM_WORDS = len(freq_dict)\n",
    "tag_list = list(tags.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition = {}\n",
    "emission = {}\n",
    "\n",
    "for tag in transition_counts:\n",
    "    for next_tag in transition_counts[tag]:\n",
    "        transition[f\"('{tag}', '{next_tag}')\"] = transition_counts[tag][next_tag] / tags[tag]['trans_freq']\n",
    "\n",
    "for tag in emission_counts:\n",
    "    for word in emission_counts[tag]:\n",
    "        emission[f\"('{tag}', '{word}')\"] = emission_counts[tag][word] / tags[tag]['frequency']\n",
    "\n",
    "hmm_json = {\n",
    "    'transition': transition,\n",
    "    'emission': emission,\n",
    "}\n",
    "\n",
    "with open(OUTPUT_PATH_HMM, 'w') as json_file:\n",
    "    json.dump(hmm_json, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition parameters:  1351\n",
      "Emission parameters:  30303\n"
     ]
    }
   ],
   "source": [
    "print(f\"Transition parameters:  {len(transition)}\")\n",
    "print(f\"Emission parameters:  {len(emission)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many transition and emission parameters in your HMM?\n",
    "\n",
    "Transition parameters:  1351\n",
    "Emission parameters:  30303"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Greedy Decoding with HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store in np arrays for quicker processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_prob = np.zeros(NUM_TAGS)\n",
    "for o, tag in enumerate(tags):\n",
    "    initial_prob[o] = initial_counts.get(tag, 0) / len(train_data)\n",
    "\n",
    "transition_prob = np.zeros((NUM_TAGS, NUM_TAGS))\n",
    "for tag in transition_counts:\n",
    "    for next_tag in transition_counts[tag]:\n",
    "        transition_prob[tags[tag]['index']][tags[next_tag]['index']] = transition_counts[tag][next_tag] / tags[tag]['trans_freq']\n",
    "\n",
    "emission_prob = np.zeros((NUM_TAGS, NUM_WORDS))\n",
    "for tag in emission_counts:\n",
    "    for word in emission_counts[tag]:\n",
    "        emission_prob[tags[tag]['index']][freq_dict[word]['index']] = emission_counts[tag][word] / tags[tag]['frequency']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the accuracy of the HMM model with the greedy algorithm on the dev data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of greedy algorithm on dev data: 0.9350297492562686\n"
     ]
    }
   ],
   "source": [
    "with open(DEV_DATA_PATH) as f:\n",
    "    dev_data = json.load(f)\n",
    "\n",
    "greedy = []\n",
    "res = np.array([], dtype=bool)\n",
    "\n",
    "for data_idx, test_entry in enumerate(dev_data):\n",
    "    sentence = test_entry['sentence']\n",
    "    pred = []\n",
    "    for o, word in enumerate(sentence):\n",
    "        init_prob = initial_prob if o == 0 else transition_prob[tags[pred[-1]]['index']]\n",
    "        mul_value = init_prob * emission_prob[:, freq_dict.get(word, freq_dict[UNKNOWN_KEY])['index']]\n",
    "        pred.append(tag_list[np.argmax(mul_value)])\n",
    "    greedy.append({\n",
    "            'index': data_idx,\n",
    "            'sentence': sentence,\n",
    "            'labels': pred\n",
    "        })\n",
    "    res = np.append(res, np.array(pred) == np.array(test_entry['labels']))\n",
    "print(f\"Accuracy of greedy algorithm on dev data: {res.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the accuracy on the dev data? 0.9350297492562686"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the predictions on test set to greedy.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TEST_DATA_PATH) as f:\n",
    "    test_data = json.load(f)\n",
    "greedy = []\n",
    "\n",
    "for data_idx, test_entry in enumerate(test_data):\n",
    "    sentence = test_entry['sentence']\n",
    "    pred = []\n",
    "    for o, word in enumerate(sentence):\n",
    "        init_prob = initial_prob if o == 0 else transition_prob[tags[pred[-1]]['index']]\n",
    "        mul_value = init_prob * emission_prob[:, freq_dict.get(word, freq_dict[UNKNOWN_KEY])['index']]\n",
    "        pred.append(tag_list[np.argmax(mul_value)])\n",
    "    greedy.append({\n",
    "            'index': data_idx,\n",
    "            'sentence': sentence,\n",
    "            'labels': pred\n",
    "        })\n",
    "with open(OUTPUT_PATH_GREEDY, 'w') as json_file:\n",
    "    json.dump(greedy, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Viterbi Decoding with HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(sentence, initial_prob, transition_prob, emission_prob):\n",
    "\n",
    "    text_length = len(sentence)\n",
    "    viterbi_matrix = np.zeros((NUM_TAGS, text_length))\n",
    "    backpointers = np.zeros((NUM_TAGS, text_length), dtype=int)\n",
    "\n",
    "    word = sentence[0] if sentence[0] in freq_dict else UNKNOWN_KEY\n",
    "    word_index = freq_dict[word]['index']\n",
    "    viterbi_matrix[:, 0] = initial_prob * emission_prob[:, word_index]\n",
    "\n",
    "    for t in range(1, len(sentence)):\n",
    "        for i, tag in enumerate(tags):\n",
    "            word = sentence[t] if sentence[t] in freq_dict else UNKNOWN_KEY\n",
    "            word_index = freq_dict[word]['index']\n",
    "\n",
    "            max_prob = 0\n",
    "            max_index = 0\n",
    "            for j, prev_tag in enumerate(tags):\n",
    "                transition = transition_prob[j, i]\n",
    "                emission = emission_prob[i, word_index]\n",
    "                prob = viterbi_matrix[j, t - 1] * transition * emission\n",
    "\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    max_index = j\n",
    "\n",
    "            viterbi_matrix[i, t] = max_prob\n",
    "            backpointers[i, t] = max_index\n",
    "\n",
    "    # Backtrack to find the best sequence of tags\n",
    "    best_sequence = []\n",
    "    max_prob = 0\n",
    "    max_index = 0\n",
    "    for i in range(NUM_TAGS):\n",
    "        if viterbi_matrix[i, len(sentence) - 1] > max_prob:\n",
    "            max_prob = viterbi_matrix[i, len(sentence) - 1]\n",
    "            max_index = i\n",
    "\n",
    "    best_sequence.append(max_index)\n",
    "    for t in range(len(sentence) - 1, 0, -1):\n",
    "        max_index = backpointers[max_index, t]\n",
    "        best_sequence.append(max_index)\n",
    "\n",
    "    best_sequence = best_sequence[::-1]  # Reverse the sequence\n",
    "\n",
    "    return [tag_list[i] for i in best_sequence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform POS tagging using Viterbi algorithm and calculate accuracy on dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of viterbi algorithm on dev data: 0.9476959504583814\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "true_labels = []\n",
    "viterbi_res = []\n",
    "for data_idx, test_entry in enumerate(dev_data):\n",
    "    sentence = test_entry['sentence']\n",
    "    true_label = test_entry['labels']\n",
    "    predicted_label = viterbi(sentence, initial_prob, transition_prob, emission_prob)\n",
    "    viterbi_res.append({\n",
    "            'index': data_idx,\n",
    "            'sentence': sentence,\n",
    "            'labels': predicted_label\n",
    "        })\n",
    "    predictions.extend(predicted_label)\n",
    "    true_labels.extend(true_label)\n",
    "\n",
    "accuracy = (np.array(predictions) == np.array(true_labels)).mean()\n",
    "print(\"Accuracy of viterbi algorithm on dev data:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of viterbi algorithm on dev data: 0.9476959504583814"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the predictions on test set to viterbi.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TEST_DATA_PATH) as f:\n",
    "    test_data = json.load(f)\n",
    "viterbi_res = []\n",
    "for data_idx, test_entry in enumerate(test_data):\n",
    "    sentence = test_entry['sentence']\n",
    "    predicted_label = viterbi(sentence, initial_prob, transition_prob, emission_prob)\n",
    "    viterbi_res.append({\n",
    "            'index': data_idx,\n",
    "            'sentence': sentence,\n",
    "            'labels': predicted_label\n",
    "        })\n",
    "\n",
    "with open(OUTPUT_PATH_VITERBI, 'w') as json_file:\n",
    "    json.dump(viterbi_res, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
