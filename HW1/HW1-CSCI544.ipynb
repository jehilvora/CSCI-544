{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 544 HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=Warning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data from the file using the read_table method from Pandas. Piazza mentions that a .csv file is available in the grader's environment but as that data is in a tsv format I have kept it as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data = pd.read_table('data.tsv', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select only the `star_rating` and `review_body` columns from the data. Also drop any rows that have an invalid value for star_rating. i.e. 1 <= `star_rating` <= 5 using a combination of coerce parameter on to_numeric and the dropna method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data = review_data[['star_rating', 'review_body']]\n",
    "review_data['star_rating'] = pd.to_numeric(review_data['star_rating'], errors='coerce')\n",
    "review_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## We form two classes and select 50000 reviews randomly from each class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_reviews = review_data[review_data['star_rating'] >= 4]\n",
    "negative_reviews = review_data[review_data['star_rating'] < 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "NUM_ROWS = 50000\n",
    "reviews_subset = pd.concat([positive_reviews.sample(NUM_ROWS, random_state=SEED), negative_reviews.sample(NUM_ROWS, random_state=SEED)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply cleaning by\n",
    "- removing links using BeautifulSoup\n",
    "- removing non-alphabetical characters\n",
    "- performing contractions with the `contractions` library\n",
    "- removing extra spaces\n",
    "- converting all characters to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review_text(text):\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    text = contractions.fix(text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_subset['cleaned_text'] = reviews_subset['review_body'].apply(clean_review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314.24925,299.44586\n"
     ]
    }
   ],
   "source": [
    "avg_length_before_cleaning = reviews_subset['review_body'].apply(len).mean()\n",
    "avg_length_after_cleaning = reviews_subset['cleaned_text'].apply(len).mean()\n",
    "print(f'{avg_length_before_cleaning},{avg_length_after_cleaning}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the data by removing stop words and lemmatizing the words. I remove the stop words defined in the `stopwords` corpus and I have performed lemmatization using the `WordNetLemmatizer` from the `nltk` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    words = nltk.word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "    return ' '.join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_subset['preprocessed_review'] = reviews_subset['cleaned_text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299.44586,184.04841\n"
     ]
    }
   ],
   "source": [
    "avg_length_before_preprocessing = reviews_subset['cleaned_text'].apply(len).mean()\n",
    "avg_length_after_preprocessing = reviews_subset['preprocessed_review'].apply(len).mean()\n",
    "print(f'{avg_length_before_preprocessing},{avg_length_after_preprocessing}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF and BoW Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our features are engineered using the `tf-idf` and `BoW` methods. Our target values are split into classes 1 (1 <= `star_rating` <= 3) and 2 (4 <= `star_rating` <= 5). Perform an 80-20 split of the data into training and test sets which gives us 80000 training samples and 20000 test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = reviews_subset['preprocessed_review']\n",
    "y = (reviews_subset['star_rating'] >= 4).astype(int) + 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While performing feature engineering make sure to call fit_transform on the training data and transform on the test data to ensure that we prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer()\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
    "X_test_bow = bow_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Using Both Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7898182528825484 0.8111389864525841 0.8003366503292244\n",
      "0.7778559228373355 0.7769192172604115 0.7773872878803092\n"
     ]
    }
   ],
   "source": [
    "perceptron_tfidf = Perceptron()\n",
    "perceptron_bow = Perceptron()\n",
    "\n",
    "perceptron_tfidf.fit(X_train_tfidf, y_train)\n",
    "perceptron_bow.fit(X_train_bow, y_train)\n",
    "\n",
    "y_pred_tfidf = perceptron_tfidf.predict(X_test_tfidf)\n",
    "y_pred_bow = perceptron_bow.predict(X_test_bow)\n",
    "\n",
    "precision_tfidf = precision_score(y_test, y_pred_tfidf)\n",
    "recall_tfidf = recall_score(y_test, y_pred_tfidf)\n",
    "f1_tfidf = f1_score(y_test, y_pred_tfidf)\n",
    "\n",
    "precision_bow = precision_score(y_test, y_pred_bow)\n",
    "recall_bow = recall_score(y_test, y_pred_bow)\n",
    "f1_bow = f1_score(y_test, y_pred_bow)\n",
    "\n",
    "print(f'{precision_bow} {recall_bow} {f1_bow}')\n",
    "print(f'{precision_tfidf} {recall_tfidf} {f1_tfidf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Using Both Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8408687480421844 0.8081284495735073 0.8241735748643946\n",
      "0.8438438438438438 0.845960863020572 0.8449010273114508\n"
     ]
    }
   ],
   "source": [
    "svm_model_tfidf = LinearSVC()\n",
    "svm_model_bow = LinearSVC()\n",
    "\n",
    "svm_model_tfidf.fit(X_train_tfidf, y_train)\n",
    "svm_model_bow.fit(X_train_bow, y_train)\n",
    "\n",
    "y_pred_tfidf = svm_model_tfidf.predict(X_test_tfidf)\n",
    "y_pred_bow = svm_model_bow.predict(X_test_bow)\n",
    "\n",
    "precision_tfidf = precision_score(y_test, y_pred_tfidf)\n",
    "recall_tfidf = recall_score(y_test, y_pred_tfidf)\n",
    "f1_tfidf = f1_score(y_test, y_pred_tfidf)\n",
    "\n",
    "precision_bow = precision_score(y_test, y_pred_bow)\n",
    "recall_bow = recall_score(y_test, y_pred_bow)\n",
    "f1_bow = f1_score(y_test, y_pred_bow)\n",
    "\n",
    "print(f'{precision_bow} {recall_bow} {f1_bow}')\n",
    "print(f'{precision_tfidf} {recall_tfidf} {f1_tfidf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Using Both Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have provided a `max_iter` value of 1000 here to prevent the `ConvergenceWarning` from being thrown and to ensure that the model converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8332532897896455 0.8644743398106627 0.8485767387264012\n",
      "0.8552511877084807 0.8431489785749875 0.8491569650742673\n"
     ]
    }
   ],
   "source": [
    "lr_model_tfidf = LogisticRegression(max_iter=1000)\n",
    "lr_model_bow = LogisticRegression(max_iter=1000)\n",
    "\n",
    "lr_model_tfidf.fit(X_train_tfidf, y_train)\n",
    "lr_model_bow.fit(X_train_bow, y_train)\n",
    "\n",
    "y_pred_tfidf = lr_model_tfidf.predict(X_test_tfidf)\n",
    "y_pred_bow = lr_model_bow.predict(X_test_bow)\n",
    "\n",
    "precision_tfidf = precision_score(y_test, y_pred_tfidf)\n",
    "recall_tfidf = recall_score(y_test, y_pred_tfidf)\n",
    "f1_tfidf = f1_score(y_test, y_pred_tfidf)\n",
    "\n",
    "precision_bow = precision_score(y_test, y_pred_bow)\n",
    "recall_bow = recall_score(y_test, y_pred_bow)\n",
    "f1_bow = f1_score(y_test, y_pred_bow)\n",
    "\n",
    "print(f'{precision_bow} {recall_bow} {f1_bow}')\n",
    "print(f'{precision_tfidf} {recall_tfidf} {f1_tfidf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Using Both Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.837139689578714 0.7577521324636227 0.7954701079799842\n",
      "0.8113885919735692 0.8379327646763672 0.8244470774091628\n"
     ]
    }
   ],
   "source": [
    "nb_model_tfidf = MultinomialNB()\n",
    "nb_model_bow = MultinomialNB()\n",
    "\n",
    "nb_model_tfidf.fit(X_train_tfidf, y_train)\n",
    "nb_model_bow.fit(X_train_bow, y_train)\n",
    "\n",
    "y_pred_tfidf = nb_model_tfidf.predict(X_test_tfidf)\n",
    "y_pred_bow = nb_model_bow.predict(X_test_bow)\n",
    "\n",
    "precision_tfidf = precision_score(y_test, y_pred_tfidf)\n",
    "recall_tfidf = recall_score(y_test, y_pred_tfidf)\n",
    "f1_tfidf = f1_score(y_test, y_pred_tfidf)\n",
    "\n",
    "precision_bow = precision_score(y_test, y_pred_bow)\n",
    "recall_bow = recall_score(y_test, y_pred_bow)\n",
    "f1_bow = f1_score(y_test, y_pred_bow)\n",
    "\n",
    "print(f'{precision_bow} {recall_bow} {f1_bow}')\n",
    "print(f'{precision_tfidf} {recall_tfidf} {f1_tfidf}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
